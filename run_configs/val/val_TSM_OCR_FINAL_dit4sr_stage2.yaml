

init:
  seed: 42


data:
  train:
    name: 
    hq_img_path: 
    ann_path: 
    hq_prompt_path: 
    null_text_ratio: 

  val:
    # eval_list: [realtext, satext_lv3, satext_lv2, satext_lv1]
    eval_list: [satext_lv1]

    realtext:
      lq_img_path: /mnt/dataset1/text_restoration/tair_published/real_text/LQ
      hq_img_path: /mnt/dataset1/text_restoration/tair_published/real_text/HQ
      ann_path: /mnt/dataset1/text_restoration/tair_published/real_text/realtext_ann.json
      vlm_captioner: 
      vlm_input_ques: 
      # vlm_caption_path: result_vlm/orig_dit4sr/realtext/llava13b_prompt
      # vlm_caption_path: result_vlm/lq_caption/realtext/realtext_Englishques0/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/realtext/realtext_Englishques1/qwenvl_7b
      vlm_caption_path: result_vlm/lq_caption/realtext/realtext_Englishques2/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/realtext/realtext_Englishques3/qwenvl_7b
      val_num_img: 847
      # val_img_id: []
      # val_img_id: [Canon_001_HR_crop_1, Canon_21_x4_5_crop_0, Canon_24_x4_21_crop_0, Canon_30_x4_44_crop_0, Canon_31_x4_52_crop_0, Canon_32_x4_21_crop_0, Canon_32_x4_23_crop_0, Canon_33_x4_33_crop_0, Canon_37_x4_45_crop_0, Canon_41_x4_32_crop_0, Canon_41_x4_35_crop_0, DSC_0972_x4_13_crop_0, DSC_0984_x4_2_crop_0, DSC_0984_x4_6_crop_0, DSC_1023_x4_16_crop_0, DSC_1330_x4_4_crop_0, DSC_1342_x4_6_crop_0,  ]
      val_img_id: [DSC_1278_x4_47_crop_0,  DSC_1330_x4_26_crop_1, IMG_227_x4_6_crop_0, P1160768_x4_5_crop_0, panasonic_151_x4_40_crop_0]


    satext_lv3:
      lq_img_path: /mnt/dataset1/text_restoration/SAMText_test_degradation/lv3
      hq_img_path: /mnt/dataset1/text_restoration/100K/test
      ann_path: /mnt/dataset1/text_restoration/100K/test/dataset.json
      vlm_captioner:
      vlm_input_ques: 
      # vlm_caption_path: result_vlm/orig_dit4sr/satext_lv3/llava13b_prompt
      # vlm_caption_path: result_vlm/lq_caption/satext_lv3/satext_lv3_Englishques0/qwenvl_7b
      vlm_caption_path: result_vlm/lq_caption/satext_lv3/satext_lv3_Englishques1/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/satext_lv3/satext_lv3_Englishques2/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/satext_lv3/satext_lv3_Englishques3/qwenvl_7b
      val_num_img: 1000
      val_img_id: [sa_93833_crop_0, sa_95236_crop_1, sa_833252_crop_1, sa_835211_crop_0, sa_854982_crop_0]

  
    satext_lv2:
      lq_img_path: /mnt/dataset1/text_restoration/SAMText_test_degradation/lv2
      hq_img_path: /mnt/dataset1/text_restoration/100K/test
      ann_path: /mnt/dataset1/text_restoration/100K/test/dataset.json
      vlm_captioner: 
      vlm_input_ques: 
      # vlm_caption_path: result_vlm/orig_dit4sr/satext_lv2/llava13b_prompt
      # vlm_caption_path: result_vlm/lq_caption/satext_lv2/satext_lv2_Englishques0/qwenvl_7b
      vlm_caption_path: result_vlm/lq_caption/satext_lv2/satext_lv2_Englishques1/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/satext_lv2/satext_lv2_Englishques2/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/satext_lv2/satext_lv2_Englishques3/qwenvl_7b
      val_num_img: 1000
      val_img_id: [sa_940610_crop_1]


    satext_lv1:
      lq_img_path: /mnt/dataset1/text_restoration/SAMText_test_degradation/lv1
      hq_img_path: /mnt/dataset1/text_restoration/100K/test
      ann_path: /mnt/dataset1/text_restoration/100K/test/dataset.json
      vlm_captioner: llava_13b
      vlm_input_ques: 0
      # vlm_caption_path: result_vlm/orig_dit4sr/satext_lv1/llava13b_prompt
      vlm_caption_path: result_vlm/lq_caption/satext_lv1/satext_lv1_Englishques0/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/satext_lv1/satext_lv1_Englishques1/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/satext_lv1/satext_lv1_Englishques2/qwenvl_7b
      # vlm_caption_path: result_vlm/lq_caption/satext_lv1/satext_lv1_Englishques3/qwenvl_7b
      val_num_img: 1000
      val_img_id: [sa_943565_crop_1, sa_940610_crop_1]


    # choose from [gt, pred_tsm, pred_vlm, null]
    # text_cond_prompt: "gt"
    # text_cond_prompt: "null"
    text_cond_prompt: "pred_tsm"
    # text_cond_prompt: "pred_vlm"



    vlm:
      vlm_correction: False 
      model: qwenvl-7b
      # vlm_apply_at_iter: [5, 10, 15, 20, 25, 30, 35]

      vlm_apply_at_iter: [10]
      # vlm_apply_at_iter: [10, 20]

      # vlm_apply_at_iter: [20]
      # vlm_apply_at_iter: [20, 25, 30, 35]
      # vlm_apply_at_iter: [20, 30]

      # vlm_apply_at_iter: [30]

      tsm_apply_at_iter:



    ocr:
      vis_ocr: True
      # vis_timestep: [-1] # visualize all timesteps
      # vis_timestep: [0, 1, 2, 3, 4, 5,  10,11,12,13,14,15, 20,21,22,23,24,25, 30,31,32,33,34,35, 39]  # visualize selected time iter
      vis_timestep: [0, 5, 10, 15, 20, 25, 30, 35, 39]  # visualize selected time iter
    

    attn:
      vis_map: False


    eval_region: all


    added_prompt:
    negative_prompt: 
    # added_prompt: Cinematic, hyper sharpness, highly detailed, perfect without deformations, camera, hyper detailed photo - realistic maximum detail, 32k, Color Grading, ultra HD, extreme meticulous detailing, skin pore detailing.
    # negative_prompt: motion blur, noisy, dotted, bokeh, pointed, CG Style, 3D render, unreal engine, blurring, dirty, messy, worst quality, low quality, frames, watermark, signature, jpeg artifacts, deformed, lowres, chaotic
    save_prompts: True
    sample_times: 1
    
    guidance_scale: 1.0
    # guidance_scale: 8.0

    start_point: noise
    
    latent_tiled_size: 64
    latent_tiled_overlap: 24
    upscale: 4
    process_size: 512
    num_inference_steps: 40

    # choose from [adain, wavelet]
    align_method: adain
    # align_method: wavelet
    

model:
  noise_scheduler:
    weighting_scheme: 
    logit_mean: 
    logit_std: 
    mode_scale: 
    precondition_outputs:

  dit:
    name: 
    resolution: 
    load_precomputed_caption: 
    use_gtprompt: 
    text_condition:
      # caption_style: tag
      caption_style: descriptive

  ts_module:
    name:  


ckpt:
  init_path:
    vae: preset/models/stable-diffusion-3.5-medium
    noise_scheduler: preset/models/stable-diffusion-3.5-medium
    tokenizer: preset/models/stable-diffusion-3.5-medium
    text_encoder: preset/models/stable-diffusion-3.5-medium
    dit: 
    ts_module: 
  resume_path:
    # dit: result_train/FINAL_dit4sr/fp16__stage1__dit4sr-5e-05__bs-2__gradaccum-32__finetune-lqbranch/checkpoint-8000
    
    dit: result_train/JIHYE_CKPT/jihye_stage1/checkpoint-32000
    ts_module: result_train/stage2_STABLE_REPA_dit4sr_testr/stable-repa-0-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23__fp16__stage2__testr__1e-05__bs-1__gradaccum-1____ocrloss0.02__descriptive__feat-hq-lq__hidden-after__ir-dit4sr-jihye-s1/checkpoint-40000/ts_module0040000.pt
    
    # dit: result_train/FINAL_FINAL_dit4sr/fp16__stage3__dit4sr-1e-05__testr-1e-05__ocrloss1.0__extract-hqlq_feat-num-24__bs-1__gradaccum-64__FINAL_CVPR_DETACH/checkpoint-500
    # ts_module: result_train/FINAL_FINAL_dit4sr/fp16__stage3__dit4sr-1e-05__testr-1e-05__ocrloss1.0__extract-hqlq_feat-num-24__bs-1__gradaccum-64__FINAL_CVPR_DETACH/checkpoint-500/ts_module0000500.pt





    # # stage2__finetune-lq-tsm-frozen__ckpt2k
    # dit: result_train/FINAL_FINAL_dit4sr/fp16__stage2__dit4sr-1e-05__testr-1e-05__ocrloss0.01__extract-hqlq_feat-num-24__bs-1__gradaccum-64__finetune-lq__ir-dit4sr-s1-jihye__tsm-FROZEN-s2-40k/checkpoint-2000
    # ts_module: result_train/FINAL_FINAL_dit4sr/fp16__stage2__dit4sr-1e-05__testr-1e-05__ocrloss0.01__extract-hqlq_feat-num-24__bs-1__gradaccum-64__finetune-lq__ir-dit4sr-s1-jihye__tsm-FROZEN-s2-40k/checkpoint-2000/ts_module0002000.pt

    # # stage2__finetune-lq-hq-tsm-frozen__ckpt2k
    # dit: result_train/FINAL_FINAL_dit4sr/fp16__stage2__dit4sr-1e-05__testr-1e-05__ocrloss0.01__extract-hqlq_feat-num-24__bs-1__gradaccum-64__finetune-lq-hq__ir-dit4sr-s1-jihye__tsm-FROZEN-s2-40k/checkpoint-2000
    # ts_module: result_train/FINAL_FINAL_dit4sr/fp16__stage2__dit4sr-1e-05__testr-1e-05__ocrloss0.01__extract-hqlq_feat-num-24__bs-1__gradaccum-64__finetune-lq-hq__ir-dit4sr-s1-jihye__tsm-FROZEN-s2-40k/checkpoint-2000/ts_module0002000.pt




train:
  stage: stage2
  mixed_precision: 'fp16'
  model: ['transformer', 'ts_module']


  transformer:
    # choose from ["dit4sr", "dit4sr_ocrbranch_ocr2hq", "dit4sr_ocrbranch_ocr2hq2ocr"]
    architecture: "dit4sr"
    ocr_branch_init: 
    lr: 
    finetune_layer_names: []
    # choose from ['hq_feat', 'lq_feat', 'ocr_feat', 'hqlq_feat']
    feat_extract: hqlq_feat
    # feat_extract: lq_feat
    feat_extract_layer: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]



  ts_module:
    architecture: testr
    lr: 
    finetune_layer_names: []
    


  num_train_epochs: 
  batch_size:        
  num_workers: 
  gradient_accumulation_steps: 
  max_train_steps: 
  lr_scheduler: 
  lr_warmup_steps:
  lr_num_cycles: 
  lr_power: 
  max_grad_norm: 
  set_grads_to_none: 
  scale_lr: 
  use_8bit_adam: 
  ocr_loss_weight: 
  

val:
  val_every_step: 


save:
  # output_dir: ./result_val_FINAL_dit4sr/full_eval
  # output_dir: ./result_val_FINAL_FINAL_dit4sr/full_eval
  output_dir: ./result_val_FINAL_FINAL_dit4sr/few_eval_TSM
  checkpointing_steps:


log:
  tracker: 
    report_to: wandb
    key: e32eed0c2509bf898b850b0065ab62345005fb73
    project_name: cvpr26_tair2_val
    server: 20
    gpu: 3
    # msg: "FINAL__ir-dit4sr-s1-8k"
    # msg: "FINAL__ir-dit4sr-s1-32k"

    # msg: "STAGE1__finetune-lq-hq__ckpt-jihye__gt-prompt"
    # msg: "STAGE1__finetune-lq-hq__ckpt-jihye__vlm-prompt-ques0-qwen7b"
    # msg: "STAGE1__finetune-lq-hq__ckpt-jihye__vlm-prompt-ques1-qwen7b"
    # msg: "STAGE1__finetune-lq-hq__ckpt-jihye__vlm-prompt-ques2-qwen7b"
    # msg: "STAGE1__finetune-lq-hq__ckpt-jihye__vlm-prompt-ques3-qwen7b"


    # msg: "stage2__FINAL-vlm-qwenvl-7b-correct-10__cfg1"
    # msg: "stage2__FINAL-vlm-qwenvl-7b-correct-20__cfg1"
    # msg: "stage2__FINAL-vlm-qwenvl-7b-correct-30__cfg1"
    # msg: "stage2__FINAL-vlm-qwenvl-7b-correct-20-25-30-35__cfg1"
    # msg: "stage2__FINAL-vlm-qwenvl-7b-correct-10-20__cfg1"
    # msg: "stage2__FINAL-vlm-qwenvl-7b-correct-20-30__cfg1"



    # msg: "stage2__FINAL-vlm-ques2-qwenvl-7b-correct-10__cfg8-added-prompt"
    # msg: "stage2__FINAL-vlm-ques2-qwenvl-7b-correct-10__cfg8"

    # msg: "stage2__FINAL-vlm-ques2-qwenvl-7b-correct-10-20__cfg8"



    # msg: "stage2__FINAL-vlm-ques2-qwenvl-7b-correct-20__cfg1"


    msg: "stage2__FINAL__cfg1__TSM-step-0-5-10-15-20-25-30-25-39__FINALSAMPLE"
    # msg: "stage2__FINAL__vlm-ques2-qwenvl-7b-correct-10__cfg1__TSM-step-0-5-10-15-20-25-30-25-39__FINALSAMPLE"



    # msg: "stage2__FINAL__finetune-ir-lq-tsm-frozen__ckpt2k__vlm-prompt-ques2-qwenvl-7b"
    # msg: "stage2__FINAL__finetune-ir-lq-hq-tsm-frozen__ckpt2k__vlm-prompt-ques2-qwenvl-7b"

    

  log_dir: logs
