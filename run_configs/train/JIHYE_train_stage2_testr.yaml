

init:
  seed: 42


data:
  train:
    name: satext
    # ----------------------------------
    #      STAGE2 - MODIFY HERE ! 
    # ----------------------------------
    # hq_img_path: /PATH/TO/100K/train        # 아래에 예시 경로 설정했습니다
    # ann_path: /PATH/TO/train/dataset.json   # # 아래에 예시 경로 설정했습니다    
    hq_img_path: /mnt/dataset1/text_restoration/100K/train
    ann_path: /mnt/dataset1/text_restoration/100K/train/dataset.json
    hq_prompt_path: 
    null_text_ratio: 0.1

  val:
    eval_list: []

    realtext:
      lq_img_path: 
      hq_img_path: 
      ann_path: 
      vlm_captioner: 
      vlm_input_ques: 
      vlm_caption_path: 
      val_num_img: 
      val_img_id: []

    satext_lv3:
      lq_img_path: 
      hq_img_path: 
      ann_path: 
      vlm_captioner: 
      vlm_input_ques: 
      vlm_caption_path: 
      val_num_img: 
      val_img_id: []
  
    satext_lv2:
      lq_img_path: 
      hq_img_path: 
      ann_path: 
      vlm_captioner: 
      vlm_input_ques: 
      vlm_caption_path: 
      val_num_img:
      val_img_id: []

    satext_lv1:
      lq_img_path: 
      hq_img_path: 
      ann_path: 
      vlm_captioner: 
      vlm_input_ques: 
      vlm_caption_path: 
      val_num_img: 
      val_img_id: []

    # choose from [gt, pred_tsm, pred_vlm, null]
    text_cond_prompt: 

    ocr:
      vis_ocr: False
      vis_timestep: []

    attn:
      vis_map: False
      
    eval_region: 

    added_prompt: 
    negative_prompt: 
    save_prompts: 
    sample_times: 
    guidance_scale: 
    start_point: 
    latent_tiled_size: 
    latent_tiled_overlap: 
    upscale: 
    process_size: 
    num_inference_steps: 
    align_method: 

model:
  noise_scheduler:
    weighting_scheme: logit_normal
    logit_mean: 0.0
    logit_std: 1.0 
    mode_scale: 1.29
    precondition_outputs: 1

  dit:
    name: dit4sr
    resolution: 512
    load_precomputed_caption: True
    use_gtprompt: True
    text_condition:
      # caption_style: tag
      caption_style: descriptive

  ts_module:
    name: testr 


ckpt:
  init_path:
    vae: preset/models/stable-diffusion-3.5-medium
    noise_scheduler: preset/models/stable-diffusion-3.5-medium
    tokenizer: preset/models/stable-diffusion-3.5-medium
    text_encoder: preset/models/stable-diffusion-3.5-medium
    dit: preset/models/dit4sr/dit4sr_q
    ts_module: preset/models/testr/totaltext_testr_R_50_polygon.pth
  resume_path:
    # ----------------------------------
    #      STAGE2 - MODIFY HERE ! 
    # ----------------------------------
    # dit: PATH/TO/STAGE1/CKPT/ 아래에 예시 경로 설정했습니다
    dit: 
    # dit: result_train/JIHYE_stage1/fp16__stage1__dit4sr-5e-05__bs-1__gradaccum-1__jihye/checkpoint-40
    ts_module: 


train:
  stage: stage2
  mixed_precision: fp16
  model: ['transformer', 'ts_module']

  transformer:
    # choose from ["dit4sr", "dit4sr_ocrbranch_ocr2hq", "dit4sr_ocrbranch_ocr2hq2ocr"]
    architecture: dit4sr
    ocr_branch_init: 
    lr: 5e-5
    # finetuning only the LQ branch (control branch)
    finetune_layer_names: [
      'control_conv',
      'to_q_control', 'to_k_control', 'to_v_control', 'to_out_control'
    ]
    feat_extract: hqlq_feat
    feat_extract_layer: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]


  ts_module:
    architecture: testr
    lr: 5e-5
    finetune_layer_names: []

  # ----------------------------------
  #      STAGE2 - MODIFY HERE ! 
  # ----------------------------------
  num_train_epochs: 10  # 60k 일단 10에폭으로 돌려봐서 얼마나 걸리는지 봐야할 것 같아요  
  batch_size: 4         # 배치사이즈는 h100에 최대 올라가는걸로 세팅해주세요
  num_workers: 4
  gradient_accumulation_steps: 16  # grad accum도 선택적으로 하셔도 될것같아요 terediff 했던것처럼
  
  
  max_train_steps: 
  lr_scheduler: "constant"
  lr_warmup_steps: 0
  lr_num_cycles: 1
  lr_power: 1.0
  max_grad_norm: 1.0
  set_grads_to_none: False
  scale_lr: False
  use_8bit_adam: False
  ocr_loss_weight: 1.0
  

val:
  val_every_step: 


# ----------------------------------
#      STAGE2 - MODIFY HERE ! 
# ----------------------------------
save:
  output_dir: ./result_train/JIHYE_stage2       # ckpt 저장 경로 설정해주세요
  checkpointing_steps: 500                    # 500마다 저장하는걸로 일단 정했어요


log:
  tracker: 
    report_to: 
    key: 
    project_name: 
    # 참고로 아래는 저의 wandb 로깅 세팅입니다
    # report_to: wandb
    # key: e32eed0c2509bf898b850b0065ab62345005fb73
    # project_name: cvpr26_tair_extension
    server: samsung
    gpu: 1234
    msg: jihye
  log_dir: logs